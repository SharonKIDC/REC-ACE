{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9531a0d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b6cd4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import T5Tokenizer\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b206c5e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df89725b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scores_histogram(data, dataset=None, bins=10, figsize=(10,8)):\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.hist(data, bins=bins, edgecolor='black')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title(f'Coinfidence Score Histogram for {dataset}')\n",
    "    return fig\n",
    "\n",
    "def check_if_word_in_vocab(words, vocab, tokenizer):\n",
    "    words_as_tokens = []\n",
    "    in_vocab = []\n",
    "    for word in words:\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        words_as_tokens.extend(tokens)\n",
    "        in_vocab.extend([int(token.lower() in vocab.keys()) for token in tokens])\n",
    "    return {'tokens': words_as_tokens, 'in_vocab': in_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ffeae2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EasyData():\n",
    "    def __init__(self, data_path: str):\n",
    "        self.path = data_path\n",
    "        self.data_list = self._load_json(data_path)\n",
    "        self.num_samples = len(self.data_list)\n",
    "        \n",
    "        self.all_gt_words = []\n",
    "        self.all_words = []\n",
    "        self.all_scores = []\n",
    "        self.all_rights = []\n",
    "        \n",
    "        self._enroll_data()\n",
    "        \n",
    "    def _load_json(self, path: str)-> list:\n",
    "        with open(path) as fp:\n",
    "            data = json.load(fp)\n",
    "        assert not data is None, f\"Somethings went wrong when trying to load: {self.path}\"\n",
    "        return data\n",
    "    \n",
    "    def _enroll_data(self):\n",
    "        print('enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)')\n",
    "        for datapoint in tqdm(self.data_list):\n",
    "            self.all_gt_words.extend([word.lower() for word in datapoint['truth'].split(' ')])\n",
    "            asr = datapoint['asr']\n",
    "            for word, score, right in asr:\n",
    "                self.all_words.append(word.lower())\n",
    "                self.all_scores.append(score)\n",
    "                self.all_rights.append(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95181f98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37128fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36379a80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t5tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e12480",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get Tokenizer's vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d81fe08a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = t5tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702b52c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98a57dc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Train Clean': '../data/default/train_clean.json',\n",
    "    'Default Train Other': '../data/default/train_other.json',\n",
    "    'Default Dev Clean': '../data/default/dev_clean.json',\n",
    "    'Default Dev Other': '../data/default/dev_other.json',\n",
    "    'Default Test clean': '../data/default/test_clean.json',\n",
    "    'Default Test Other': '../data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Train Clean': '../data/video/train_clean.json',\n",
    "    'Video Train Other': '../data/video/train_other.json',\n",
    "    'Video Dev Clean': '../data/video/dev_clean.json',\n",
    "    'Video Dev Other': '../data/video/dev_other.json',\n",
    "    'Video Test Clean': '../data/video/test_clean.json',\n",
    "    'Video Test Other': '../data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12bddeb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_path = '../data/data_exploring'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "143f6362",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103895/103895 [00:00<00:00, 104756.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146550/146550 [00:01<00:00, 117860.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2697/2697 [00:00<00:00, 98061.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2809/2809 [00:00<00:00, 154171.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2615/2615 [00:00<00:00, 185723.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2925/2925 [00:00<00:00, 192218.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104013/104013 [00:01<00:00, 100803.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148678/148678 [00:01<00:00, 104541.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [00:00<00:00, 179958.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2864/2864 [00:00<00:00, 197382.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [00:00<00:00, 153172.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolling all data into lists: all_gt_words, all_words, all_scores, all_rights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2939/2939 [00:00<00:00, 204337.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for ds_name, ds_path in datasets_dict.items():\n",
    "    \n",
    "    #make output dir\n",
    "    curr_fold_name =  ds_path.split('data')[-1].split('.')[0][1:] +'/'\n",
    "    curr_save_path = os.path.join(output_path, curr_fold_name)\n",
    "    os.makedirs(curr_save_path, exist_ok=True)\n",
    "    # Enroll all words\n",
    "    data_explorer= EasyData(data_path=ds_path)\n",
    "    # Coinfidence instogram\n",
    "    scores_fig = plot_scores_histogram(data_explorer.all_scores, dataset=ds_name, bins=20) \n",
    "    plt.savefig(os.path.join(curr_save_path, f'confidence_histogram.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    X_words = check_if_word_in_vocab(data_explorer.all_words, vocab=vocab, tokenizer=t5tokenizer)\n",
    "    X_df = pd.DataFrame.from_dict(X_words)\n",
    "    X_df_count = X_df['in_vocab'].value_counts()\n",
    "    X_df_count = X_df_count.reset_index()\n",
    "    X_df_count['HUE'] = 'Input Words'\n",
    "    \n",
    "    Y_words = check_if_word_in_vocab(data_explorer.all_gt_words, vocab=vocab, tokenizer=t5tokenizer)\n",
    "    Y_df = pd.DataFrame.from_dict(Y_words)\n",
    "    Y_df_count = Y_df['in_vocab'].value_counts()\n",
    "    Y_df_count = Y_df_count.reset_index()\n",
    "    Y_df_count['HUE'] = 'Labels'\n",
    "    \n",
    "    concat = pd.concat([X_df_count, Y_df_count])\n",
    "    mapping = {0: 'Not in Vocab', 1: 'In Vocab'}\n",
    "    concat['index'] = concat['index'].map(mapping)\n",
    "    ax = sns.barplot(data=concat, x=\"index\", y=\"in_vocab\", hue=\"HUE\")\n",
    "    ax.set_title(f'Words in T5 Vocab for {ds_name}')\n",
    "    plt.savefig(os.path.join(curr_save_path, f'words_in_vocab.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39809717",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/data_exploring/video/test_other/'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_fold_name =  ds_path.split('data')[-1].split('.')[0][1:] +'/'\n",
    "os.path.join(output_path, curr_fold_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}