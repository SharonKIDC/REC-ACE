{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4f9352",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b69b5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.autonotebook import tqdm as tqdm_notebook\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from data_utils.dataset import prepare_data\n",
    "from data_utils.utils import read_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4d00b",
   "metadata": {},
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f532e285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105f961f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the random seed for pandas\n",
    "# pandas gets its random seed from numpy, so using numpy's seed will affect pandas\n",
    "\n",
    "# Set the random seed for NLTK\n",
    "# NLTK gets its random seed from the Python random number generator (using random.seed())\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422b232",
   "metadata": {},
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38e9663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Train Clean': 'data/default/train_clean.json',\n",
    "    'Default Train Other': 'data/default/train_other.json',\n",
    "    'Default Dev Clean': 'data/default/dev_clean.json',\n",
    "    'Default Dev Other': 'data/default/dev_other.json',\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Train Clean': 'data/video/train_clean.json',\n",
    "    'Video Train Other': 'data/video/train_other.json',\n",
    "    'Video Dev Clean': 'data/video/dev_clean.json',\n",
    "    'Video Dev Other': 'data/video/dev_other.json',\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d638f7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d991cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068e355",
   "metadata": {},
   "source": [
    "## Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d034af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 't5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04623fc8",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315802c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bbd2a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373f052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd108a00",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa01feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_json(json_path=datasets_dict['Default Train Clean'])\n",
    "dev_set = read_json(json_path=datasets_dict['Default Dev Clean'])\n",
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf505",
   "metadata": {},
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf812b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 103895/103895 [00:09<00:00, 10425.14it/s]\n",
      "100%|██████████████████████████████████████████| 2697/2697 [00:00<00:00, 15090.89it/s]\n",
      "100%|██████████████████████████████████████████| 2615/2615 [00:00<00:00, 15048.03it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_loader = prepare_data(data=train_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = prepare_data(data=dev_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "test_loader = prepare_data(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e532ce7",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662e5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372bc6e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "928826cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, n_epochs, train_data, dev_data, optimizer, criterion):\n",
    "    \"\"\" Training loop for the model\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Model to train\n",
    "            n_epochs (int): Number of epochs to train\n",
    "            train_data (DataLoader): DataLoader with train data\n",
    "            dev_data (DataLoader): DataLoader with dev data\n",
    "            optimizer (torch.optim): Optimizer for the model\n",
    "            criterion (torch.nn): Loss function\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with train and dev losses and accuracies\n",
    "    \"\"\"\n",
    "\n",
    "    # metrics placeholder for recording training stats\n",
    "    metrics = {\n",
    "        'loss': {\n",
    "            'train': [],\n",
    "            'dev':   []\n",
    "        },\n",
    "        'acc': {\n",
    "            'train': [],\n",
    "            'dev':   []\n",
    "        }\n",
    "    }\n",
    "    pbar = tqdm(range(n_epochs), position=0, desc=f\"\\tEpoch: {1}/{n_epochs}\")\n",
    "    for epoch in pbar:\n",
    "\n",
    "        train_losses, train_acc = [], []\n",
    "        dev_losses, dev_acc = [], []\n",
    "\n",
    "        ### TRAIN\n",
    "        model.train()\n",
    "\n",
    "        # Iterating over batches in train data\n",
    "        pbar_train = tqdm_notebook(train_data, position=1)\n",
    "        for i_batch, batch in enumerate(pbar_train):\n",
    "            pbar_train.set_description(f\"Training on batch: {i_batch+1}/{len(train_data)}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X = batch['sentences'].to(DEVICE)\n",
    "            y = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss = model(input_ids=X, labels=y).loss\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ### Evaluate DEV set\n",
    "        model.eval()\n",
    "\n",
    "        # No need for gradients when evaluating\n",
    "        with torch.no_grad():\n",
    "            pbar_dev = tqdm_notebook(dev_data, position=2)\n",
    "            for i_batch, batch in enumerate(pbar_dev):\n",
    "                pbar_dev.set_description(f\"DEV Iteration: {i_batch+1}/{len(dev_data)}\")\n",
    "\n",
    "\n",
    "                X = batch['sentences'].to(DEVICE)\n",
    "                y = batch['labels'].to(DEVICE)\n",
    "\n",
    "                loss = model(input_ids=X, labels=y).loss\n",
    "       \n",
    "                # Calculate DEV loss\n",
    "                loss = criterion(preds, y_hot)\n",
    "                dev_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Collect epoch's avg scores\n",
    "        metrics['loss']['train'].append(np.mean(train_losses))\n",
    "        metrics['loss']['dev'].append(np.mean(dev_losses))\n",
    "\n",
    "        pbar.set_description(f\"\\tEpoch: {epoch+1}/{n_epochs}, \\t Train Loss AVG: {metrics['loss']['train'][-1]:.04}, Dev Loss AVG: {metrics['loss']['dev'][-1]:.04}\")\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a8c503d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fdec0d47204041b325b2f2d1680d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\tEpoch: 1/20:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5df96b3efd40de9fe9ab7703988ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdev_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, n_epochs, train_data, dev_data, optimizer, criterion)\u001b[0m\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mX, labels\u001b[38;5;241m=\u001b[39my)\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     48\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m### Evaluate DEV set\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mac/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mac/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model, metrics = training(model=model,\n",
    "                          n_epochs=20,\n",
    "                          train_data=train_loader,\n",
    "                          dev_data=dev_loader,\n",
    "                          optimizer=optimizer,\n",
    "                          criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc493df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
