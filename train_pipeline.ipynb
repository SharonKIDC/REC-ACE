{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4f9352",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69b5430",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from data_utils.dataset import prepare_data\n",
    "from data_utils.utils import read_json\n",
    "from models.rec_ace import RecACEWrapModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4d00b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f532e285",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x110fb2e70>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set the random seed for pandas\n",
    "# pandas gets its random seed from numpy, so using numpy's seed will affect pandas\n",
    "\n",
    "# Set the random seed for NLTK\n",
    "# NLTK gets its random seed from the Python random number generator (using random.seed())\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422b232",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e9663e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Train Clean': 'data/default/train_clean.json',\n",
    "    'Default Train Other': 'data/default/train_other.json',\n",
    "    'Default Dev Clean': 'data/default/dev_clean.json',\n",
    "    'Default Dev Other': 'data/default/dev_other.json',\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Train Clean': 'data/video/train_clean.json',\n",
    "    'Video Train Other': 'data/video/train_other.json',\n",
    "    'Video Dev Clean': 'data/video/dev_clean.json',\n",
    "    'Video Dev Other': 'data/video/dev_other.json',\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d638f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d991cad0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e93148",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068e355",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d034af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "# Base architecture\n",
    "t5_type = 't5-small'\n",
    "\n",
    "# What do we train - original / rec_ace\n",
    "model_type = 'rec_ace'\n",
    "\n",
    "# How to quantize the confidence vectors [only required for rec_ac]\n",
    "bin_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04623fc8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315802c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd108a00",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa01feb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = read_json(json_path=datasets_dict['Default Train Clean'])\n",
    "dev_set = read_json(json_path=datasets_dict['Default Dev Clean'])\n",
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf505",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf812b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Mode - using only 10390 out of 103895, training datapoints\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_loader = prepare_data(data=train_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=True, debug=debug)\n",
    "# train_loader = prepare_data(data=dev_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = prepare_data(data=dev_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "test_loader = prepare_data(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "RecACEWrapModel(\n  (model): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 512)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 512)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n                (relative_attention_bias): Embedding(32, 8)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (2): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (3): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (4): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (5): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 512)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n                (relative_attention_bias): Embedding(32, 8)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (2): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (3): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (4): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (5): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=512, bias=False)\n                (k): Linear(in_features=512, out_features=512, bias=False)\n                (v): Linear(in_features=512, out_features=512, bias=False)\n                (o): Linear(in_features=512, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=512, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n  )\n  (rec_ace_block): RecAceEmbeddingBlock(\n    (words_emb): Embedding(32130, 512, padding_idx=0)\n    (scores_emb): Embedding(12, 512, padding_idx=0)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecACEWrapModel(t5_type=t5_type, model_type=model_type, bin_size=bin_size)\n",
    "model.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662e5796",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372bc6e6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928826cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training(model, n_epochs, train_data, dev_data, optimizer, criterion):\n",
    "    \"\"\" Training loop for the model\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Model to train\n",
    "            n_epochs (int): Number of epochs to train\n",
    "            train_data (DataLoader): DataLoader with train data\n",
    "            dev_data (DataLoader): DataLoader with dev data\n",
    "            optimizer (torch.optim): Optimizer for the model\n",
    "            criterion (torch.nn): Loss function\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with train and dev losses and accuracies\n",
    "    \"\"\"\n",
    "\n",
    "    # metrics placeholder for recording training stats\n",
    "    metrics = {\n",
    "        'loss': {\n",
    "            'train': [],\n",
    "            'dev':   []\n",
    "        },\n",
    "        'acc': {\n",
    "            'train': [],\n",
    "            'dev':   []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f'Training model of type: {model.model_type}')\n",
    "    print('- Scores vector will be ' + ('ignored' if model.model_type == 'original' else 'used'))\n",
    "    pbar = tqdm(range(n_epochs), position=0)\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}/{n_epochs}\")\n",
    "\n",
    "        train_losses, train_acc = [], []\n",
    "        dev_losses, dev_acc = [], []\n",
    "\n",
    "        ### TRAIN\n",
    "        model.train()\n",
    "\n",
    "        # Iterating over batches in train data\n",
    "        pbar_train = tqdm(train_data, desc=\"Train\")\n",
    "        for batch in pbar_train:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X = batch['sentences'].to(DEVICE)\n",
    "            S = batch['scores'].to(DEVICE)\n",
    "            y = batch['labels'].to(DEVICE)\n",
    "            loss = model(input_ids=X, labels=y, scores_ids=S).loss\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ### Evaluate DEV set\n",
    "        model.eval()\n",
    "\n",
    "        # No need for gradients when evaluating\n",
    "        with torch.no_grad():\n",
    "            pbar_dev = tqdm(dev_data, desc=\"Dev\")\n",
    "            for batch in pbar_dev:\n",
    "\n",
    "                X = batch['sentences'].to(DEVICE)\n",
    "                S = batch['scores'].to(DEVICE)\n",
    "                y = batch['labels'].to(DEVICE)\n",
    "\n",
    "                loss = model(input_ids=X, labels=y, scores_ids=S).loss\n",
    "       \n",
    "                # Calculate DEV loss\n",
    "                dev_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Collect epoch's avg scores\n",
    "        metrics['loss']['train'].append(np.mean(train_losses))\n",
    "        metrics['loss']['dev'].append(np.mean(dev_losses))\n",
    "\n",
    "        print(f\"\\tDone Epoch: {epoch+1}/{n_epochs}, \\t Train Loss AVG: {metrics['loss']['train'][-1]:.04}, Dev Loss AVG: {metrics['loss']['dev'][-1]:.04}\")\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model of type: rec_ace\n",
      "- Scores vector will be used\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e637d5ee7ba4349ad231f90a23e19b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train:   0%|          | 0/1299 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57a4220ec10d4783908c7283913803dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m----> 2\u001B[0m model, metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mdev_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdev_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                          \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36mtraining\u001B[0;34m(model, n_epochs, train_data, dev_data, optimizer, criterion)\u001B[0m\n\u001B[1;32m     49\u001B[0m     loss \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39mX, labels\u001B[38;5;241m=\u001B[39my, scores_ids\u001B[38;5;241m=\u001B[39mS)\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     51\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m---> 52\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m### Evaluate DEV set\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mac/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mac/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model, metrics = training(model=model,\n",
    "                          n_epochs=20,\n",
    "                          train_data=train_loader,\n",
    "                          dev_data=dev_loader,\n",
    "                          optimizer=optimizer,\n",
    "                          criterion=criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}