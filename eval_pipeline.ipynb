{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline\n",
    "---\n",
    "## To download our best achieved results, please refer to the instructions provided in the README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from data_utils.dataset import prepare_data_basic, prepare_data_for_prompt_engineering\n",
    "from data_utils.utils import read_json\n",
    "from models.rec_ace import RecACEWrapModel, detokenize_and_clean\n",
    "from evaluation import Evaluator, calculate_exact_match, calculate_wer, BERTS\n",
    "\n",
    "# BERT Score model warm-up\n",
    "BERTS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x182c47e4250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Base architecture\n",
    "t5_type = 't5-small'\n",
    "\n",
    "# How to quantize the confidence vectors [only required for rec_ac]\n",
    "bin_size=10\n",
    "\n",
    "results_dir = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])\n",
    "test_set_other = read_json(json_path=datasets_dict['Default Test Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "test_loader = prepare_data_basic(data=test_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "test_loader_other = prepare_data_basic(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define metrics for evaluation:\n",
    "1. WER - Word Error Rate\n",
    "1. EM - Exact Match\n",
    "1. BS - BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_metrics = ['wer', 'em', 'bs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, metrics=eval_metrics, data_loaders={'Clean': test_loader, 'Other': test_loader_other}):\n",
    "\n",
    "    evaluators = {}\n",
    "    data_types = list(data_loaders.keys())\n",
    "\n",
    "    ### Evaluate TEST set\n",
    "    model.eval()\n",
    "\n",
    "    for data_type in data_types:\n",
    "        evaluator = Evaluator(metrics=metrics, set_types=['test'])\n",
    "\n",
    "        # No need for gradients when evaluating\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loaders[data_type]:\n",
    "\n",
    "                X = batch['sentences'].to(DEVICE)\n",
    "                S = batch['scores'].to(DEVICE)\n",
    "                y = batch['labels'].to(DEVICE)\n",
    "\n",
    "                test_preds = model(input_ids=X, labels=y, scores_ids=S)\n",
    "                test_logits = test_preds.logits\n",
    "\n",
    "                test_reference = detokenize_and_clean(tokenizer, y)\n",
    "                test_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "                \n",
    "                evaluator.calculate_metrics(set_type='test', reference=test_reference, predicted=test_predicted)\n",
    "\n",
    "        evaluator.end_epoch_routine(print_metrics=False)\n",
    "        evaluators[data_type] = evaluator\n",
    "\n",
    "    return {data_type: evaluator.metrics_df for data_type, evaluator in evaluators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics_comp_table(evals_df_dict, title='Metrics Comparison'):\n",
    "    # merge all dfs from evals_df_dict, use the keys as an index. Each df has only one line so it's ok\n",
    "    # don't use the old index\n",
    "    evals_df = pd.concat(evals_df_dict.values(), keys=evals_df_dict.keys()).reset_index(level=1, drop=True)\n",
    "\n",
    "    print(title)\n",
    "    print(tabulate(evals_df, headers='keys', tablefmt='psql', floatfmt='.4f'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating metrics for the ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR evaluation on Clean datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+-------+\n",
      "|    |   wer |    em |    bs |\n",
      "|----+-------+-------+-------|\n",
      "|  1 | 0.124 | 0.288 | 0.914 |\n",
      "+----+-------+-------+-------+\n",
      "\n",
      "\n",
      "Running ASR evaluation on Other datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+-------+\n",
      "|    |   wer |    em |    bs |\n",
      "|----+-------+-------+-------|\n",
      "|  1 | 0.272 | 0.136 | 0.812 |\n",
      "+----+-------+-------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asr_evaluators = {}\n",
    "\n",
    "for data_type in ['Clean', 'Other']:\n",
    "\n",
    "    # Print data type header\n",
    "    print(f'Running ASR evaluation on {data_type} datasets')\n",
    "\n",
    "    asr_evaluator = Evaluator(metrics=eval_metrics, set_types=['test'])\n",
    "\n",
    "    for batch in (test_loader if data_type == 'Clean' else test_loader_other):\n",
    "        reference = detokenize_and_clean(tokenizer, batch['labels'])\n",
    "        predicted = detokenize_and_clean(tokenizer, batch['sentences'])\n",
    "        asr_evaluator.calculate_metrics(set_type='test', reference=reference, predicted=predicted)\n",
    "\n",
    "    asr_evaluator.end_epoch_routine(print_metrics=False)\n",
    "\n",
    "    # Print final metrics\n",
    "    asr_evaluator.print_final_metrics()\n",
    "\n",
    "    # Save results to disk\n",
    "    dir_path = os.path.join(results_dir, 'ASR', data_type)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    asr_evaluator.store_df(dir_path)\n",
    "\n",
    "    # Save evaluator for later use\n",
    "    asr_evaluators[data_type] = asr_evaluator\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Best Models, Evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rec-ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 21\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Clean/2023-08-23_01-46-03'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rec_ace_results = evaluate_model_performance(rec_ace_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 34\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Clean/2023-08-27_01-50-22'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "t5_results = evaluate_model_performance(t5_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rec-ACE (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 49\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Other/2023-08-24_01-05-43'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rec_ace_other_results = evaluate_model_performance(rec_ace_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## T5 (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 48\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Other/2023-08-24_17-27-55'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_other_results = evaluate_model_performance(t5_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model T5 trained on prompt-engineered data (\"Clean\" dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "p_test_loader = prepare_data_for_prompt_engineering(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "p_test_loader_other = prepare_data_for_prompt_engineering(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 19\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_t5_prompt_Clean/2023-08-26_10-58-53'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_prompt_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_prompt_results = evaluate_model_performance(t5_prompt_best_model, data_loaders={'Clean': p_test_loader, 'Other': p_test_loader_other})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Set Results\n",
      "+--------------------------+--------+--------+--------+\n",
      "|                          |    wer |     em |     bs |\n",
      "|--------------------------+--------+--------+--------|\n",
      "| ASR                      | 0.1237 | 0.2880 | 0.9137 |\n",
      "| RecAce(trained on Clean) | 0.1052 | 0.3545 | 0.9179 |\n",
      "| T5(trained on Clean)     | 0.1057 | 0.3471 | 0.9189 |\n",
      "| RecAce(trained on Other) | 0.0904 | 0.3778 | 0.9253 |\n",
      "| T5(trained on Other)     | 0.0937 | 0.3591 | 0.9227 |\n",
      "| T5P(trained on Clean)    | 0.1106 | 0.3361 | 0.9148 |\n",
      "+--------------------------+--------+--------+--------+\n",
      "\n",
      "Other Test Set Results\n",
      "+--------------------------+--------+--------+--------+\n",
      "|                          |    wer |     em |     bs |\n",
      "|--------------------------+--------+--------+--------|\n",
      "| ASR                      | 0.2722 | 0.1356 | 0.8123 |\n",
      "| RecAce(trained on Clean) | 0.2512 | 0.1574 | 0.8095 |\n",
      "| T5(trained on Clean)     | 0.2526 | 0.1506 | 0.8087 |\n",
      "| RecAce(trained on Other) | 0.2256 | 0.1720 | 0.8239 |\n",
      "| T5(trained on Other)     | 0.2302 | 0.1646 | 0.8209 |\n",
      "| T5P(trained on Clean)    | 0.2555 | 0.1530 | 0.8075 |\n",
      "+--------------------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for set_type in ['Clean', 'Other']:\n",
    "    print_metrics_comp_table({\n",
    "        'ASR':                      asr_evaluators[set_type].metrics_df['test'],\n",
    "        'RecAce(trained on Clean)': rec_ace_results[set_type]['test'],\n",
    "        'T5(trained on Clean)':     t5_results[set_type]['test'],\n",
    "        'RecAce(trained on Other)': rec_ace_other_results[set_type]['test'],\n",
    "        'T5(trained on Other)':     t5_other_results[set_type]['test'],\n",
    "        'T5P(trained on Clean)':    t5_prompt_results[set_type]['test'],\n",
    "    }, title=f'{set_type} Test Set Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "how_much_to_print = 20\n",
    "min_wer_diff = 0.4 # ASR WER - REC WER\n",
    "rec_ace_max_wer = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "exa_batch_size = 1\n",
    "exa_loader = prepare_data_basic(data=test_set , tokenizer=tokenizer, batch_size=exa_batch_size, shuffle=False)\n",
    "exa_loader_other = prepare_data_basic(data=test_set_other , tokenizer=tokenizer, batch_size=exa_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "examples_model = rec_ace_other_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1:\n",
      "- Reference:\n",
      "\t\"the feverish colour came into her cheek and the feverish flame into her eye\"\n",
      "- ASR hypothesis:\n",
      "\t\"favorite color came into a cheeks and the feverish flame into a r i\" (WER=0.5714)\n",
      "- RED-ACE:\n",
      "\t\"the feverish colour came into her cheeks the feverish flame into her eye\" (WER=0.1429)\n",
      "--------------------\n",
      "Example #2:\n",
      "- Reference:\n",
      "\t\"exactly here replied the brahman\"\n",
      "- ASR hypothesis:\n",
      "\t\"exactly hair reply to carmen\" (WER=0.8000)\n",
      "- RED-ACE:\n",
      "\t\"exactly here replied the carzenman\" (WER=0.2000)\n",
      "--------------------\n",
      "Example #3:\n",
      "- Reference:\n",
      "\t\"from about two thousand b c\"\n",
      "- ASR hypothesis:\n",
      "\t\"i'm about 2,000 bc\" (WER=0.8333)\n",
      "- RED-ACE:\n",
      "\t\"and about two thousand b c\" (WER=0.1667)\n",
      "--------------------\n",
      "Example #4:\n",
      "- Reference:\n",
      "\t\"fourteen ninety nine\"\n",
      "- ASR hypothesis:\n",
      "\t\"1499\" (WER=1.0000)\n",
      "- RED-ACE:\n",
      "\t\"fourteen ninety nine\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #5:\n",
      "- Reference:\n",
      "\t\"confectionary fifteen o eight\"\n",
      "- ASR hypothesis:\n",
      "\t\"confectionery 1508\" (WER=1.0000)\n",
      "- RED-ACE:\n",
      "\t\"confectionery fifteen o eight\" (WER=0.2500)\n",
      "--------------------\n",
      "Example #6:\n",
      "- Reference:\n",
      "\t\"and thus they journeyed onwards a long long way\"\n",
      "- ASR hypothesis:\n",
      "\t\"unless they journeyed on words a long long way\" (WER=0.4444)\n",
      "- RED-ACE:\n",
      "\t\"and thus they journeyed onwards a long long way\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #7:\n",
      "- Reference:\n",
      "\t\"how why\"\n",
      "- ASR hypothesis:\n",
      "\t\"how y\" (WER=0.5000)\n",
      "- RED-ACE:\n",
      "\t\"how why\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #8:\n",
      "- Reference:\n",
      "\t\"a hundred and twenty six for admission ninety eight against\"\n",
      "- ASR hypothesis:\n",
      "\t\"126 for the admission 98 against\" (WER=0.8000)\n",
      "- RED-ACE:\n",
      "\t\"onea hundred and sixty six for the ninety eight against\" (WER=0.3000)\n",
      "--------------------\n",
      "Example #9:\n",
      "- Reference:\n",
      "\t\"they hated pleasure\"\n",
      "- ASR hypothesis:\n",
      "\t\"they hate it pleasure\" (WER=0.6667)\n",
      "- RED-ACE:\n",
      "\t\"they hated pleasure\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #10:\n",
      "- Reference:\n",
      "\t\"very well at half past ten\"\n",
      "- ASR hypothesis:\n",
      "\t\"very well at 10:30\" (WER=0.5000)\n",
      "- RED-ACE:\n",
      "\t\"very well at half past ten\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #11:\n",
      "- Reference:\n",
      "\t\"when it was the five hundred and fifty ninth night\"\n",
      "- ASR hypothesis:\n",
      "\t\"when it was the 559th not\" (WER=0.6000)\n",
      "- RED-ACE:\n",
      "\t\"when it was the five hundred and fifty ninth night\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #12:\n",
      "- Reference:\n",
      "\t\"when it was the five hundred and sixty first night\"\n",
      "- ASR hypothesis:\n",
      "\t\"when it was the 560 first not\" (WER=0.5000)\n",
      "- RED-ACE:\n",
      "\t\"when it was the five hundred and sixty first night\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #13:\n",
      "- Reference:\n",
      "\t\"a quarter past ten half past\"\n",
      "- ASR hypothesis:\n",
      "\t\"10:15 10:30\" (WER=1.0000)\n",
      "- RED-ACE:\n",
      "\t\"ten quarter past ten  past \" (WER=0.3333)\n",
      "--------------------\n",
      "Example #14:\n",
      "- Reference:\n",
      "\t\"archy checked himself and the boy laughed\"\n",
      "- ASR hypothesis:\n",
      "\t\"aren't you checked himself in the boy left\" (WER=0.5714)\n",
      "- RED-ACE:\n",
      "\t\"i checked himself and the boy laughed\" (WER=0.1429)\n",
      "--------------------\n",
      "Example #15:\n",
      "- Reference:\n",
      "\t\"my brother james was born january fifteenth eighteen forty eight john in eighteen fifty one and robert in december eighteen fifty three\"\n",
      "- ASR hypothesis:\n",
      "\t\"my brother james was born january 15-18 48 john and 1851 and robert and december 1853\" (WER=0.5455)\n",
      "- RED-ACE:\n",
      "\t\"my brother james was born january fifteen eight eighteen forty eight john and eighteen fifty one and robert in december eighteen fifty three\" (WER=0.1364)\n",
      "--------------------\n",
      "Example #16:\n",
      "- Reference:\n",
      "\t\"a thousand interest at ten per cent a week standard right\"\n",
      "- ASR hypothesis:\n",
      "\t\"1000 interest at 10% a week standard right\" (WER=0.4545)\n",
      "- RED-ACE:\n",
      "\t\"a thousand interest at ten per cent a week standard right\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #17:\n",
      "- Reference:\n",
      "\t\"leslie opened the door\"\n",
      "- ASR hypothesis:\n",
      "\t\"please leave open the door\" (WER=0.7500)\n",
      "- RED-ACE:\n",
      "\t\"t opened the door\" (WER=0.2500)\n",
      "--------------------\n",
      "Example #18:\n",
      "- Reference:\n",
      "\t\"every student is to be in east hall at half past eight\"\n",
      "- ASR hypothesis:\n",
      "\t\"if a student has to be in east whole 8:30\" (WER=0.6667)\n",
      "- RED-ACE:\n",
      "\t\"student has to be in east hall half half past eight\" (WER=0.2500)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "for batch in exa_loader_other:\n",
    "    X = batch['sentences'].to(DEVICE)\n",
    "    S = batch['scores'].to(DEVICE)\n",
    "    y = batch['labels'].to(DEVICE)\n",
    "\n",
    "    test_preds = examples_model(input_ids=X, labels=y, scores_ids=S)\n",
    "    test_logits = test_preds.logits\n",
    "\n",
    "    ex_hypothesis = detokenize_and_clean(tokenizer, X)\n",
    "    ex_reference = detokenize_and_clean(tokenizer, y)\n",
    "    ex_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "\n",
    "    rec_ace_wer = calculate_wer(ex_reference, ex_predicted)\n",
    "    asr_wer = calculate_wer(ex_reference, ex_hypothesis)\n",
    "\n",
    "    if asr_wer - rec_ace_wer > min_wer_diff and rec_ace_wer < rec_ace_max_wer:\n",
    "        ii+=1\n",
    "        print(f'Example #{ii}:')\n",
    "        print(f'- Reference:\\n\\t\"{ex_reference[0]}\"')\n",
    "        print(f'- ASR hypothesis:\\n\\t\"{ex_hypothesis[0]}\" (WER={asr_wer:.4f})')\n",
    "        print(f'- RED-ACE:\\n\\t\"{ex_predicted[0]}\" (WER={rec_ace_wer:.4f})')\n",
    "        print('-' * 20)\n",
    "\n",
    "    if ii == how_much_to_print:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pack models and results in a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dev_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote test_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote train_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote dev_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote test_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote train_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote dev_metrics.csv to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote epoch_21.pt to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote train_metrics.csv to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote dev_metrics.csv to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote epoch_34.pt to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote train_metrics.csv to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote dev_metrics.csv to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote epoch_49.pt to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote train_metrics.csv to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote dev_metrics.csv to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote epoch_48.pt to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote train_metrics.csv to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote dev_metrics.csv to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n",
      "Wrote epoch_19.pt to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n",
      "Wrote train_metrics.csv to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def write_to_zip(zipf, folder_path, _file):\n",
    "    file_path = os.path.join(folder_path, _file)\n",
    "    zipf.write(file_path, arcname=file_path)\n",
    "\n",
    "def zip_csv_files(directories, zip_filename):\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for directory in directories:\n",
    "            best_model = None\n",
    "            for folder_path, _, files in os.walk(directory):\n",
    "                for _file in files:\n",
    "                    # Write metrics files to zip\n",
    "                    if _file.endswith('.csv'):\n",
    "                        write_to_zip(zipf, folder_path, _file)\n",
    "                        print(f'Wrote {_file} to zip @ {folder_path}')\n",
    "                        continue\n",
    "                    \n",
    "                    # Get best model epoch\n",
    "                    best_model = best_model if best_model else Evaluator.get_best_epoch(directory, metric)\n",
    "\n",
    "                    # Write best model files to zip\n",
    "                    if _file.endswith(f'{best_model}.pt'):\n",
    "                        write_to_zip(zipf, folder_path, _file)\n",
    "                        print(f'Wrote {_file} to zip @ {folder_path}')\n",
    "\n",
    "# Example usage\n",
    "directories_to_zip = [\n",
    "    r'results/ASR',\n",
    "    r'results/rec_ace_Clean/2023-08-23_01-46-03',\n",
    "    r'results/original_f5_Clean/2023-08-27_01-50-22',\n",
    "    r'results/rec_ace_Other/2023-08-24_01-05-43',\n",
    "    r'results/original_f5_Other/2023-08-24_17-27-55',\n",
    "    r'results/original_t5_prompt_Clean/2023-08-26_10-58-53']\n",
    "\n",
    "output_zip_filename = 'res_and_models.zip'\n",
    "zip_csv_files(directories_to_zip, output_zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS38-Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
