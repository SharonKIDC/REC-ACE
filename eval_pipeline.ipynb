{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from evaluation import Evaluator\n",
    "from data_utils.dataset import prepare_data_basic, prepare_data_for_prompt_engineering\n",
    "from data_utils.utils import read_json\n",
    "from models.rec_ace import RecACEWrapModel, detokenize_and_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x194d2dc5230>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base architecture\n",
    "t5_type = 't5-small'\n",
    "\n",
    "# How to quantize the confidence vectors [only required for rec_ac]\n",
    "bin_size=10\n",
    "\n",
    "results_dir = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])\n",
    "test_set_other = read_json(json_path=datasets_dict['Default Test Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_loader = prepare_data_basic(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "test_loader_other = prepare_data_basic(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics for evaluation:\n",
    "1. WER - Word Error Rate\n",
    "1. EM - Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['wer', 'em']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, metrics=eval_metrics, data_loaders={'Clean': test_loader, 'Other': test_loader_other}):\n",
    "\n",
    "    evaluators = {}\n",
    "    data_types = list(data_loaders.keys())\n",
    "\n",
    "    ### Evaluate TEST set\n",
    "    model.eval()\n",
    "\n",
    "    for data_type in data_types:\n",
    "        evaluator = Evaluator(metrics=metrics, set_types=['test'])\n",
    "\n",
    "        # No need for gradients when evaluating\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loaders[data_type]:\n",
    "\n",
    "                X = batch['sentences'].to(DEVICE)\n",
    "                S = batch['scores'].to(DEVICE)\n",
    "                y = batch['labels'].to(DEVICE)\n",
    "\n",
    "                test_preds = model(input_ids=X, labels=y, scores_ids=S)\n",
    "                test_logits = test_preds.logits\n",
    "\n",
    "                test_reference = detokenize_and_clean(tokenizer, y)\n",
    "                test_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "                \n",
    "                evaluator.calculate_metrics(set_type='test', reference=test_reference, predicted=test_predicted)\n",
    "\n",
    "        evaluator.end_epoch_routine(print_metrics=False)\n",
    "        evaluators[data_type] = evaluator\n",
    "\n",
    "    return {data_type: evaluator.metrics_df for data_type, evaluator in evaluators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_comp_table(evals_df_dict, title='Metrics Comparison'):\n",
    "    # merge all dfs from evals_df_dict, use the keys as an index. Each df has only one line so it's ok\n",
    "    # don't use the old index\n",
    "    evals_df = pd.concat(evals_df_dict.values(), keys=evals_df_dict.keys()).reset_index(level=1, drop=True)\n",
    "\n",
    "    print(title)\n",
    "    print(tabulate(evals_df, headers='keys', tablefmt='psql', floatfmt='.4f'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating metrics for the ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR evaluation on Clean datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+\n",
      "|    |   wer |    em |\n",
      "|----+-------+-------|\n",
      "|  1 | 0.124 | 0.288 |\n",
      "+----+-------+-------+\n",
      "\n",
      "\n",
      "Running ASR evaluation on Other datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+\n",
      "|    |   wer |    em |\n",
      "|----+-------+-------|\n",
      "|  1 | 0.273 | 0.135 |\n",
      "+----+-------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asr_evaluators = {}\n",
    "\n",
    "for data_type in ['Clean', 'Other']:\n",
    "\n",
    "    # Print data type header\n",
    "    print(f'Running ASR evaluation on {data_type} datasets')\n",
    "\n",
    "    asr_evaluator = Evaluator(metrics=eval_metrics, set_types=['test'])\n",
    "\n",
    "    for batch in (test_loader if data_type == 'Clean' else test_loader_other):\n",
    "        reference = detokenize_and_clean(tokenizer, batch['labels'])\n",
    "        predicted = detokenize_and_clean(tokenizer, batch['sentences'])\n",
    "        asr_evaluator.calculate_metrics(set_type='test', reference=reference, predicted=predicted)\n",
    "\n",
    "    asr_evaluator.end_epoch_routine(print_metrics=False)\n",
    "\n",
    "    # Print final metrics\n",
    "    asr_evaluator.print_final_metrics()\n",
    "\n",
    "    # Save results to disk\n",
    "    dir_path = os.path.join(results_dir, 'ASR', data_type)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    asr_evaluator.store_df(dir_path)\n",
    "\n",
    "    # Save evaluator for later use\n",
    "    asr_evaluators[data_type] = asr_evaluator\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best Models, Evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rec-ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 21\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Clean/2023-08-23_01-46-03'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_ace_results = evaluate_model_performance(rec_ace_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 34\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Clean/2023-08-27_01-50-22'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_results = evaluate_model_performance(t5_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rec-ACE (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 49\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Other/2023-08-24_01-05-43'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_ace_other_results = evaluate_model_performance(rec_ace_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 48\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Other/2023-08-24_17-27-55'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_other_results = evaluate_model_performance(t5_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model T5 trained on prompt-engineered data (\"Clean\" dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "p_test_loader = prepare_data_for_prompt_engineering(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "p_test_loader_other = prepare_data_for_prompt_engineering(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 19\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_t5_prompt_Clean/2023-08-26_10-58-53'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_prompt_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_prompt_results = evaluate_model_performance(t5_prompt_best_model, data_loaders={'Clean': p_test_loader, 'Other': p_test_loader_other})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Set Results\n",
      "+--------------------------+--------+--------+\n",
      "|                          |    wer |     em |\n",
      "|--------------------------+--------+--------|\n",
      "| ASR                      | 0.1239 | 0.2875 |\n",
      "| RecAce(trained on Clean) | 0.1054 | 0.3538 |\n",
      "| T5(trained on Clean)     | 0.1060 | 0.3465 |\n",
      "| RecAce(trained on Other) | 0.0907 | 0.3773 |\n",
      "| T5(trained on Other)     | 0.0939 | 0.3586 |\n",
      "| T5P(trained on Clean)    | 0.1108 | 0.3356 |\n",
      "+--------------------------+--------+--------+\n",
      "\n",
      "Other Test Set Results\n",
      "+--------------------------+--------+--------+\n",
      "|                          |    wer |     em |\n",
      "|--------------------------+--------+--------|\n",
      "| ASR                      | 0.2726 | 0.1350 |\n",
      "| RecAce(trained on Clean) | 0.2515 | 0.1567 |\n",
      "| T5(trained on Clean)     | 0.2527 | 0.1501 |\n",
      "| RecAce(trained on Other) | 0.2257 | 0.1713 |\n",
      "| T5(trained on Other)     | 0.2304 | 0.1640 |\n",
      "| T5P(trained on Clean)    | 0.2557 | 0.1525 |\n",
      "+--------------------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for set_type in ['Clean', 'Other']:\n",
    "    print_metrics_comp_table({\n",
    "        'ASR':                      asr_evaluators[set_type].metrics_df['test'],\n",
    "        'RecAce(trained on Clean)': rec_ace_results[set_type]['test'],\n",
    "        'T5(trained on Clean)':     t5_results[set_type]['test'],\n",
    "        'RecAce(trained on Other)': rec_ace_other_results[set_type]['test'],\n",
    "        'T5(trained on Other)':     t5_other_results[set_type]['test'],\n",
    "        'T5P(trained on Clean)':    t5_prompt_results[set_type]['test'],\n",
    "    }, title=f'{set_type} Test Set Results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS38-Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
