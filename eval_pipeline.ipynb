{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline\n",
    "---\n",
    "## To download our best achieved results, please refer to the instructions provided in the README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SharonK\\.virtualenvs\\DS38-Dev\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<evaluation.BERTS at 0x182c4138c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from data_utils.dataset import prepare_data_basic, prepare_data_for_prompt_engineering\n",
    "from data_utils.utils import read_json\n",
    "from models.rec_ace import RecACEWrapModel, detokenize_and_clean\n",
    "from evaluation import Evaluator, calculate_exact_match, calculate_wer, BERTS\n",
    "\n",
    "# BERT Score model warm-up\n",
    "BERTS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x182c47e4250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Base architecture\n",
    "t5_type = 't5-small'\n",
    "\n",
    "# How to quantize the confidence vectors [only required for rec_ac]\n",
    "bin_size=10\n",
    "\n",
    "results_dir = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])\n",
    "test_set_other = read_json(json_path=datasets_dict['Default Test Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "test_loader = prepare_data_basic(data=test_set, tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "test_loader_other = prepare_data_basic(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define metrics for evaluation:\n",
    "1. WER - Word Error Rate\n",
    "1. EM - Exact Match\n",
    "1. BS - BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_metrics = ['wer', 'em', 'bs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, metrics=eval_metrics, data_loaders={'Clean': test_loader, 'Other': test_loader_other}):\n",
    "\n",
    "    evaluators = {}\n",
    "    data_types = list(data_loaders.keys())\n",
    "\n",
    "    ### Evaluate TEST set\n",
    "    model.eval()\n",
    "\n",
    "    for data_type in data_types:\n",
    "        evaluator = Evaluator(metrics=metrics, set_types=['test'])\n",
    "\n",
    "        # No need for gradients when evaluating\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loaders[data_type]:\n",
    "\n",
    "                X = batch['sentences'].to(DEVICE)\n",
    "                S = batch['scores'].to(DEVICE)\n",
    "                y = batch['labels'].to(DEVICE)\n",
    "\n",
    "                test_preds = model(input_ids=X, labels=y, scores_ids=S)\n",
    "                test_logits = test_preds.logits\n",
    "\n",
    "                test_reference = detokenize_and_clean(tokenizer, y)\n",
    "                test_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "                \n",
    "                evaluator.calculate_metrics(set_type='test', reference=test_reference, predicted=test_predicted)\n",
    "\n",
    "        evaluator.end_epoch_routine(print_metrics=False)\n",
    "        evaluators[data_type] = evaluator\n",
    "\n",
    "    return {data_type: evaluator.metrics_df for data_type, evaluator in evaluators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics_comp_table(evals_df_dict, title='Metrics Comparison'):\n",
    "    # merge all dfs from evals_df_dict, use the keys as an index. Each df has only one line so it's ok\n",
    "    # don't use the old index\n",
    "    evals_df = pd.concat(evals_df_dict.values(), keys=evals_df_dict.keys()).reset_index(level=1, drop=True)\n",
    "\n",
    "    print(title)\n",
    "    print(tabulate(evals_df, headers='keys', tablefmt='psql', floatfmt='.4f'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating metrics for the ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR evaluation on Clean datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+-------+\n",
      "|    |   wer |    em |    bs |\n",
      "|----+-------+-------+-------|\n",
      "|  1 | 0.124 | 0.288 | 0.914 |\n",
      "+----+-------+-------+-------+\n",
      "\n",
      "\n",
      "Running ASR evaluation on Other datasets\n",
      "Test Metrics:\n",
      "+----+-------+-------+-------+\n",
      "|    |   wer |    em |    bs |\n",
      "|----+-------+-------+-------|\n",
      "|  1 | 0.272 | 0.136 | 0.812 |\n",
      "+----+-------+-------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asr_evaluators = {}\n",
    "\n",
    "for data_type in ['Clean', 'Other']:\n",
    "\n",
    "    # Print data type header\n",
    "    print(f'Running ASR evaluation on {data_type} datasets')\n",
    "\n",
    "    asr_evaluator = Evaluator(metrics=eval_metrics, set_types=['test'])\n",
    "\n",
    "    for batch in (test_loader if data_type == 'Clean' else test_loader_other):\n",
    "        reference = detokenize_and_clean(tokenizer, batch['labels'])\n",
    "        predicted = detokenize_and_clean(tokenizer, batch['sentences'])\n",
    "        asr_evaluator.calculate_metrics(set_type='test', reference=reference, predicted=predicted)\n",
    "\n",
    "    asr_evaluator.end_epoch_routine(print_metrics=False)\n",
    "\n",
    "    # Print final metrics\n",
    "    asr_evaluator.print_final_metrics()\n",
    "\n",
    "    # Save results to disk\n",
    "    dir_path = os.path.join(results_dir, 'ASR', data_type)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    asr_evaluator.store_df(dir_path)\n",
    "\n",
    "    # Save evaluator for later use\n",
    "    asr_evaluators[data_type] = asr_evaluator\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Best Models, Evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rec-ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 21\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Clean/2023-08-23_01-46-03'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rec_ace_results = evaluate_model_performance(rec_ace_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 34\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Clean/2023-08-27_01-50-22'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "t5_results = evaluate_model_performance(t5_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rec-ACE (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 49\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/rec_ace_Other/2023-08-24_01-05-43'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "rec_ace_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rec_ace_other_results = evaluate_model_performance(rec_ace_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## T5 (trained on Other dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 48\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_f5_Other/2023-08-24_17-27-55'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_other_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_other_results = evaluate_model_performance(t5_other_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model T5 trained on prompt-engineered data (\"Clean\" dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "p_test_loader = prepare_data_for_prompt_engineering(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)\n",
    "p_test_loader_other = prepare_data_for_prompt_engineering(data=test_set_other , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch by the wer is 19\n"
     ]
    }
   ],
   "source": [
    "model_res_dir = r'results/original_t5_prompt_Clean/2023-08-26_10-58-53'\n",
    "metric = 'wer'\n",
    "epoch = Evaluator.get_best_epoch(model_res_dir, metric)\n",
    "print(f'Best epoch by the {metric} is {epoch}')\n",
    "\n",
    "t5_prompt_best_model = RecACEWrapModel.load_from_disk(os.path.join(model_res_dir, f'epoch_{epoch}.pt'), 't5-small', 'original', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_prompt_results = evaluate_model_performance(t5_prompt_best_model, data_loaders={'Clean': p_test_loader, 'Other': p_test_loader_other})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Set Results\n",
      "+--------------------------+--------+--------+--------+\n",
      "|                          |    wer |     em |     bs |\n",
      "|--------------------------+--------+--------+--------|\n",
      "| ASR                      | 0.1237 | 0.2880 | 0.9137 |\n",
      "| RecAce(trained on Clean) | 0.1052 | 0.3545 | 0.9179 |\n",
      "| T5(trained on Clean)     | 0.1057 | 0.3471 | 0.9189 |\n",
      "| RecAce(trained on Other) | 0.0904 | 0.3778 | 0.9253 |\n",
      "| T5(trained on Other)     | 0.0937 | 0.3591 | 0.9227 |\n",
      "| T5P(trained on Clean)    | 0.1106 | 0.3361 | 0.9148 |\n",
      "+--------------------------+--------+--------+--------+\n",
      "\n",
      "Other Test Set Results\n",
      "+--------------------------+--------+--------+--------+\n",
      "|                          |    wer |     em |     bs |\n",
      "|--------------------------+--------+--------+--------|\n",
      "| ASR                      | 0.2722 | 0.1356 | 0.8123 |\n",
      "| RecAce(trained on Clean) | 0.2512 | 0.1574 | 0.8095 |\n",
      "| T5(trained on Clean)     | 0.2526 | 0.1506 | 0.8087 |\n",
      "| RecAce(trained on Other) | 0.2256 | 0.1720 | 0.8239 |\n",
      "| T5(trained on Other)     | 0.2302 | 0.1646 | 0.8209 |\n",
      "| T5P(trained on Clean)    | 0.2555 | 0.1530 | 0.8075 |\n",
      "+--------------------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for set_type in ['Clean', 'Other']:\n",
    "    print_metrics_comp_table({\n",
    "        'ASR':                      asr_evaluators[set_type].metrics_df['test'],\n",
    "        'RecAce(trained on Clean)': rec_ace_results[set_type]['test'],\n",
    "        'T5(trained on Clean)':     t5_results[set_type]['test'],\n",
    "        'RecAce(trained on Other)': rec_ace_other_results[set_type]['test'],\n",
    "        'T5(trained on Other)':     t5_other_results[set_type]['test'],\n",
    "        'T5P(trained on Clean)':    t5_prompt_results[set_type]['test'],\n",
    "    }, title=f'{set_type} Test Set Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "how_much_to_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n",
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "exa_batch_size = 1\n",
    "exa_loader = prepare_data_basic(data=test_set , tokenizer=tokenizer, batch_size=exa_batch_size, shuffle=False)\n",
    "exa_loader_other = prepare_data_basic(data=test_set_other , tokenizer=tokenizer, batch_size=exa_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "examples_model = rec_ace_other_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1:\n",
      "- Reference:\n",
      "\t\"margaret said mister hale as he returned from showing his guest downstairs i could not help watching your face with some anxiety when mister thornton made his confession of having been a shop boy\"\n",
      "- ASR hypothesis:\n",
      "\t\"margaret said mr. hale as he returned from showing his guests downstairs but mr. thornton made his confession of having been a truck boy\" (WER=0.4412)\n",
      "- RED-ACE:\n",
      "\t\"margaret said mister hale as he returned from showing his guests downstairs buta suppose not believe but the  but the  but mister thornton made his confession of having been a truckboy\" (WER=0.3824)\n",
      "--------------------\n",
      "Example #2:\n",
      "- Reference:\n",
      "\t\"i really liked that account of himself better than anything else he said\"\n",
      "- ASR hypothesis:\n",
      "\t\"i really like that account of himself better than anything else he said\" (WER=0.0769)\n",
      "- RED-ACE:\n",
      "\t\"i really liked that account of himself better than anything else he said\" (WER=0.0000)\n",
      "--------------------\n",
      "Example #3:\n",
      "- Reference:\n",
      "\t\"i don't think mister hale you have done quite right in introducing such a person to us without telling us what he had been\"\n",
      "- ASR hypothesis:\n",
      "\t\"i didn't think mr. hale you have done quite right introducing such a person to us that telling us what he had been\" (WER=0.1667)\n",
      "- RED-ACE:\n",
      "\t\"i don't think mister hale you have done quite right  introducing such a person to us that telling us what he had been\" (WER=0.0833)\n",
      "--------------------\n",
      "Example #4:\n",
      "- Reference:\n",
      "\t\"his father speculated wildly failed and then killed himself because he could not bear the disgrace\"\n",
      "- ASR hypothesis:\n",
      "\t\"it's part of the speculation wadley failed and then killed himself because he could not bear the disgrace\" (WER=0.3750)\n",
      "- RED-ACE:\n",
      "\t\"part thed he and and then killed himself because he could not bear the disgrace\" (WER=0.3125)\n",
      "--------------------\n",
      "Example #5:\n",
      "- Reference:\n",
      "\t\"so they left milton\"\n",
      "- ASR hypothesis:\n",
      "\t\"go to the lake milton\" (WER=1.0000)\n",
      "- RED-ACE:\n",
      "\t\"to laid milton\" (WER=0.7500)\n",
      "--------------------\n",
      "Example #6:\n",
      "- Reference:\n",
      "\t\"when he spoke of the mechanical powers he evidently looked upon them only as new ways of extending trade and making money\"\n",
      "- ASR hypothesis:\n",
      "\t\"when he spoke of the mechanical power he evidently looked up on them early as new ways of extending trade and making money\" (WER=0.1818)\n",
      "- RED-ACE:\n",
      "\t\"when he spoke of the mechanical power he evidently looked upon them early as new ways of extending trade and making money\" (WER=0.0909)\n",
      "--------------------\n",
      "Example #7:\n",
      "- Reference:\n",
      "\t\"and the poor men around him they were poor because they were vicious out of the pale of his sympathies because they had not his iron nature and the capabilities that it gives him for being rich\"\n",
      "- ASR hypothesis:\n",
      "\t\"and the pullman around him they were pool because they were vicious answer the path of his sympathy because they had not he's our nature and the capabilities that it gives him for being rich\" (WER=0.2432)\n",
      "- RED-ACE:\n",
      "\t\"and the pull men around him they were poor because they were vicious answered of the path sympathy his sympathythisies because they had not his own nature and the capabilities that it gives him for being rich\" (WER=0.1622)\n",
      "--------------------\n",
      "Example #8:\n",
      "- Reference:\n",
      "\t\"improvident and self indulgent were his words\"\n",
      "- ASR hypothesis:\n",
      "\t\"pro fit in them self-indulgent where is was\" (WER=1.1429)\n",
      "- RED-ACE:\n",
      "\t\"forplodt self self indulgent where his was\" (WER=0.5714)\n",
      "--------------------\n",
      "Example #9:\n",
      "- Reference:\n",
      "\t\"just as she was leaving the room she hesitated she was inclined to make an acknowledgment which she thought would please her father but which to be full and true must include a little annoyance\"\n",
      "- ASR hypothesis:\n",
      "\t\"just as she was leaving the room she hesitated she was inclined to make an apology meant what she thought would please her father but which to be full and true mustang clearly solenoids\" (WER=0.2286)\n",
      "- RED-ACE:\n",
      "\t\"just as she was leaving the room she hesitated she was inclined to make an apologyment which she thought would please her father but which to be full and true must  clearlya clearly solesyance\" (WER=0.1429)\n",
      "--------------------\n",
      "Example #10:\n",
      "- Reference:\n",
      "\t\"papa i do think mister thornton a very remarkable man but personally i don't like him at all\"\n",
      "- ASR hypothesis:\n",
      "\t\"i do think mr. fountain a very remarkable man bless me i didn't like him at all\" (WER=0.3333)\n",
      "- RED-ACE:\n",
      "\t\"i do think mister fountainthornton a very remarkable man bless bless i don't like him at all\" (WER=0.2222)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "for batch in exa_loader_other:\n",
    "    X = batch['sentences'].to(DEVICE)\n",
    "    S = batch['scores'].to(DEVICE)\n",
    "    y = batch['labels'].to(DEVICE)\n",
    "\n",
    "    test_preds = examples_model(input_ids=X, labels=y, scores_ids=S)\n",
    "    test_logits = test_preds.logits\n",
    "\n",
    "    ex_hypothesis = detokenize_and_clean(tokenizer, X)\n",
    "    ex_reference = detokenize_and_clean(tokenizer, y)\n",
    "    ex_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "\n",
    "    rec_ace_wer = calculate_wer(ex_reference, ex_predicted)\n",
    "    asr_wer = calculate_wer(ex_reference, ex_hypothesis)\n",
    "\n",
    "    if rec_ace_wer < asr_wer:\n",
    "        ii+=1\n",
    "        print(f'Example #{ii}:')\n",
    "        print(f'- Reference:\\n\\t\"{ex_reference[0]}\"')\n",
    "        print(f'- ASR hypothesis:\\n\\t\"{ex_hypothesis[0]}\" (WER={asr_wer:.4f})')\n",
    "        print(f'- RED-ACE:\\n\\t\"{ex_predicted[0]}\" (WER={rec_ace_wer:.4f})')\n",
    "        print('-' * 20)\n",
    "\n",
    "    if ii == how_much_to_print:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pack models and results in a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dev_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote test_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote train_metrics.csv to zip @ results/ASR\\Clean\n",
      "Wrote dev_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote test_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote train_metrics.csv to zip @ results/ASR\\Other\n",
      "Wrote dev_metrics.csv to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote epoch_21.pt to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote train_metrics.csv to zip @ results/rec_ace_Clean/2023-08-23_01-46-03\n",
      "Wrote dev_metrics.csv to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote epoch_34.pt to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote train_metrics.csv to zip @ results/original_f5_Clean/2023-08-27_01-50-22\n",
      "Wrote dev_metrics.csv to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote epoch_49.pt to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote train_metrics.csv to zip @ results/rec_ace_Other/2023-08-24_01-05-43\n",
      "Wrote dev_metrics.csv to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote epoch_48.pt to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote train_metrics.csv to zip @ results/original_f5_Other/2023-08-24_17-27-55\n",
      "Wrote dev_metrics.csv to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n",
      "Wrote epoch_19.pt to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n",
      "Wrote train_metrics.csv to zip @ results/original_t5_prompt_Clean/2023-08-26_10-58-53\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def write_to_zip(zipf, folder_path, _file):\n",
    "    file_path = os.path.join(folder_path, _file)\n",
    "    zipf.write(file_path, arcname=file_path)\n",
    "\n",
    "def zip_csv_files(directories, zip_filename):\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for directory in directories:\n",
    "            best_model = None\n",
    "            for folder_path, _, files in os.walk(directory):\n",
    "                for _file in files:\n",
    "                    # Write metrics files to zip\n",
    "                    if _file.endswith('.csv'):\n",
    "                        write_to_zip(zipf, folder_path, _file)\n",
    "                        print(f'Wrote {_file} to zip @ {folder_path}')\n",
    "                        continue\n",
    "                    \n",
    "                    # Get best model epoch\n",
    "                    best_model = best_model if best_model else Evaluator.get_best_epoch(directory, metric)\n",
    "\n",
    "                    # Write best model files to zip\n",
    "                    if _file.endswith(f'{best_model}.pt'):\n",
    "                        write_to_zip(zipf, folder_path, _file)\n",
    "                        print(f'Wrote {_file} to zip @ {folder_path}')\n",
    "\n",
    "# Example usage\n",
    "directories_to_zip = [\n",
    "    r'results/ASR',\n",
    "    r'results/rec_ace_Clean/2023-08-23_01-46-03',\n",
    "    r'results/original_f5_Clean/2023-08-27_01-50-22',\n",
    "    r'results/rec_ace_Other/2023-08-24_01-05-43',\n",
    "    r'results/original_f5_Other/2023-08-24_17-27-55',\n",
    "    r'results/original_t5_prompt_Clean/2023-08-26_10-58-53']\n",
    "\n",
    "output_zip_filename = 'res_and_models.zip'\n",
    "zip_csv_files(directories_to_zip, output_zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS38-Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
