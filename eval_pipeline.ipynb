{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SharonK\\.virtualenvs\\DS38-Dev\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "from evaluation import Evaluator\n",
    "from visualization import plot_graphs\n",
    "from data_utils.dataset import prepare_data\n",
    "from data_utils.utils import read_json\n",
    "from models.rec_ace import RecACEWrapModel, detokenize_and_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x244372ccbf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set the random seed for Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set the random seed for numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\n",
    "    # Default\n",
    "    'Default Test Clean': 'data/default/test_clean.json',\n",
    "    'Default Test Other': 'data/default/test_other.json',\n",
    "    # Video\n",
    "    'Video Test Clean': 'data/video/test_clean.json',\n",
    "    'Video Test Other': 'data/video/test_other.json', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "# Base architecture\n",
    "t5_type = 't5-small'\n",
    "\n",
    "# How to quantize the confidence vectors [only required for rec_ac]\n",
    "bin_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = read_json(json_path=datasets_dict['Default Test Clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Converting the input sentences into tokens\n",
      "- Converting the GT sentences into tokens\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_loader = prepare_data(data=test_set , tokenizer=tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating metrics for the ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "+----+-------+-------+--------+--------+\n",
      "|    |   wer |    em |   bleu |   gleu |\n",
      "|----+-------+-------+--------+--------|\n",
      "|  1 | 0.129 | 0.288 |  0.760 |  0.793 |\n",
      "+----+-------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asr_evaluator = Evaluator(metrics=['wer', 'em', 'bleu', 'gleu'], set_types=types)\n",
    "\n",
    "for batch in test_loader:\n",
    "    reference = detokenize_and_clean(tokenizer, batch['sentences'])\n",
    "    predicted = detokenize_and_clean(tokenizer, batch['labels'])\n",
    "    asr_evaluator.calculate_metrics(set_type='test', reference=reference, predicted=predicted)\n",
    "\n",
    "asr_evaluator.end_epoch_routine(print_metrics=False)\n",
    "\n",
    "# Print final metrics\n",
    "asr_evaluator.print_final_metrics()\n",
    "\n",
    "# Save results to disk\n",
    "dir_path = os.path.join('results', 'ASR')\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "asr_evaluator.store_df(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load best Debug model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = Evaluator.get_best_epoch(r'results\\DebugRecAce\\2023-08-22_21-15-34')\n",
    "rec_ace_best_model = RecACEWrapModel.load_from_disk(fr'results\\DebugRecAce\\2023-08-22_21-15-34\\epoch_{epoch}.pt', 't5-small', 'rec_ace', use_pretrained=True, bin_size=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "+----+-------+-------+\n",
      "|    |   wer |    em |\n",
      "|----+-------+-------|\n",
      "|  1 | 0.279 | 0.096 |\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Evaluate TEST set\n",
    "rec_ace_best_model.eval()\n",
    "\n",
    "test_losses = []\n",
    "evaluator = Evaluator(metrics=['wer', 'em'], set_types=['test'])\n",
    "\n",
    "# No need for gradients when evaluating\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "\n",
    "        X = batch['sentences'].to(DEVICE)\n",
    "        S = batch['scores'].to(DEVICE)\n",
    "        y = batch['labels'].to(DEVICE)\n",
    "\n",
    "        test_preds = rec_ace_best_model(input_ids=X, labels=y, scores_ids=S)\n",
    "\n",
    "        test_loss = test_preds.loss\n",
    "        test_logits = test_preds.logits\n",
    "\n",
    "        test_reference = detokenize_and_clean(tokenizer, y)\n",
    "        test_predicted = detokenize_and_clean(tokenizer, test_logits.argmax(dim=-1))\n",
    "        \n",
    "        test_losses.append(test_loss.item())\n",
    "        evaluator.calculate_metrics(set_type='test', reference=test_reference, predicted=test_predicted)\n",
    "    \n",
    "    evaluator.end_epoch_routine(print_metrics=False)\n",
    "\n",
    "evaluator.print_final_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS38-Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
